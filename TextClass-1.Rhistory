# First attach the 'tm' library
# We will build a classifier for the UMICH text data
View(UMICH_training)
colnames(UMICH_training)=c("Sentiment","Text")
table(UMICH_training$Sentiment)

# First we create a corpus ...
corpus <- Corpus(VectorSource(UMICH_training$Text))

# Now we do the pre-processing of the text
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))

# We could also define our own stopwords and add to
# this list

corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)

# Now we create the DTM
dtm <- DocumentTermMatrix(corpus)

dtm
# We have about 1769 terms - let's try to reduce
sparse <- removeSparseTerms(dtm, 0.99)

# This will give us terms that appear in at least 1%
# of the documents

sparse

# We need to convert this into a dataframe and
# combine with the sentiment value we have in the
# original data.
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData = cbind(UMICH_training,important_words_df)
myData$Text=NULL
dim(myData)

# This is the data frame we can train our classifier on.
# Let's create a training and test set - here's a
# cool trick is you have the caTools package:
spl <- sample.split(myData$Sentiment, .85)

# This creates a vector which is TRUE 85% of the time
# Sentiment is T (1)
myData_train <- myData[spl==T,]
myData_test <- myData[spl==F,]
dim(myData_train)
dim(myData_test)

# First we build a simple logistic regression model
lr.model = glm(Sentiment~., data=myData_train, family=binomial)
lr.probs = predict(lr.model,newdata = myData_test, type="response")
table(myData_test$Sentiment,lr.probs>.5)

# Well that's impressive.
