# We will attempt to classify URLs using logistic regression
# First load the malicious URLs dataset

# The data set is too large - we will randomly select 10K URLs to work with
indices = sample(420464,10000)
myData = URL_data[indices,]
dim(myData)

# We need to split the urls to create "words"
myData$url=as.character(myData$url)

# We will split on the set [/,.,?,-,=] by substituting a space
myData$url=gsub("/"," ",myData$url)
myData$url=gsub("="," ",myData$url)
myData$url=gsub("-"," ",myData$url)
myData$url=gsub("."," ",myData$url, fixed = TRUE) # Note we need to escape the .
myData$url=gsub("?"," ",myData$url, fixed = TRUE) # Note we need to escape the ?
View(myData)
myData$url=gsub("_"," ",myData$url)

# Now we can create a corpus and proceed as before ...
# Use the 'tm' package
summary(myData$label)
myData$label=as.factor(myData$label)
summary(myData$label)
corpus <- Corpus(VectorSource(myData$url))

# There is not much clean up
corpus <- tm_map(corpus, content_transformer(tolower))

# Here we remove our own custom stopwords
corpus <- tm_map(corpus, removeWords, c("com","org","net"))
dtm <- DocumentTermMatrix(corpus)
sparse <- removeSparseTerms(dtm, 0.995)
sparse
train = sample(10000,7500)

# Now we create the DF ...
important_words_df <- as.data.frame(as.matrix(sparse))
colnames(important_words_df) <- make.names(colnames(important_words_df))
myData_2 = cbind(myData,important_words_df)
myData_2$url=NULL
lr.model = glm(label~., data=myData_2, subset=train,family=binomial)
lr.probs = predict(lr.model,newdata = myData_2[-train,], type="response")
table(myData_2[-train,]$label,lr.probs>.5)
summary(myData_2[-train,]$label)

# Not bad - would need to enhance
